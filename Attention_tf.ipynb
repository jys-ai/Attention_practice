{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Attention_tf.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyOYadRCeFZBqTm+yMQ28Ynw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"GEve-tNSY1gm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609730732459,"user_tz":-540,"elapsed":28541,"user":{"displayName":"송정윤","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRJcNDmEMhmBD5vygrE-8z7tCUbFYg8knVp6UC=s64","userId":"00034480537731584131"}},"outputId":"56304455-c7db-4d52-83b3-a3d4aaa8b5ed"},"source":["from google.colab import drive\n","\n","drive.mount('/content/drive', force_remount=True)\n","\n","FOLDERNAME = 'cs231n/Practice/'\n","\n","assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n","\n","%cd /content/drive/My\\ Drive\n","%cp -r $FOLDERNAME ../../\n","%cd ../../\n","%cd Practice\n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n","/content/drive/My Drive\n","/content\n","/content/Practice\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"FdLbf5zBZRJ2"},"source":["# Attention : step forward\n","Open the file `attention_layers.py`. This file implements the forward and backward passes for different types of layers that are commonly used in attention."]},{"cell_type":"code","metadata":{"id":"U2DQ4SBpZN0Q","executionInfo":{"status":"ok","timestamp":1609730732460,"user_tz":-540,"elapsed":28539,"user":{"displayName":"송정윤","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRJcNDmEMhmBD5vygrE-8z7tCUbFYg8knVp6UC=s64","userId":"00034480537731584131"}}},"source":["\n","def rel_error(x, y):\n","    \"\"\" returns relative error \"\"\"\n","    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n","\n","def softmax(a, axis = 0) :\n","    exp_a = np.exp(a)\n","    sum_exp = np.sum(exp_a, axis = axis, keepdims = True)\n","    y = exp_a / sum_exp\n","    return y\n","\n","\n","def attention_step_forward(decoder_h, encoder_h, Wc, bc, Wy, by):\n","    \n","    \"\"\"\n","    batchsize N\n","\n","    Inputs:\n","    - encoder_h : (N, T, D)\n","    - decoder_h : (N, D) at time t\n","    - Wc : (D, 2D)\n","    - bc : (D, )\n","    - Wy : (word_dim, D)\n","    - bc : (word_dim, )\n","    Returns a tuple of:\n","    - next_h: Next hidden state, of shape (N, H)\n","    - cache: Tuple of values needed for the backward pass.\n","    \"\"\"\n","    N,T,D = encoder_h.shape\n","    WD, _ = Wy.shape\n","\n","    scores, cache = None, None\n","    score_at = np.sum(encoder_h * decoder_h.reshape(N, 1, D), axis = 2)\n","    # score_at = encoder_h @ decoder_h.T\n","    prob_at =  softmax(score_at, axis = 1).reshape(N, T, 1)\n","    attention_value = np.sum(prob_at * encoder_h, axis = 1)\n","    v_t = np.concatenate([decoder_h, attention_value], axis = 1)\n","    s_t = np.tanh(v_t @ Wc.T + bc)\n","    score = softmax(s_t @ Wy.T + by, axis = 1)    \n","\n","    cache = decoder_h, encoder_h, Wc, bc, Wy, by, score_at, prob_at, attention_value, v_t, s_t, score\n","\n","    return score, cache\n","\n","def attention_step_backward(dloss, cache):\n","    \n","    \"\"\"\n","    batchsize N\n","\n","    Inputs:\n","    - encoder_h : (N, T, D)\n","    - decoder_h : (N, D) at time t\n","    - Wc : (D, 2D)\n","    - bc : (D, )\n","    - Wy : (word_dim, D)\n","    - bc : (word_dim, )\n","    Returns a tuple of:\n","    - next_h: Next hidden state, of shape (N, H)\n","    - cache: Tuple of values needed for the backward pass.\n","    \"\"\"\n","    decoder_h, encoder_h, Wc, bc, Wy, by, score_at, prob_at, attetion_value, v_t, s_t, score = cache\n","\n","    N,T,D = encoder_h.shape\n","    WD, _ = Wy.shape\n","\n","    ddh =1\n","    deh = 1\n","    dWc = 1\n","\n","\n","    # score_at = np.sum(encoder_h * decoder_h.reshape(N, 1, D), axis = 2)\n","    # prob_at =  softmax(score_at, axis = 1).reshape(N, T, 1)\n","    # attention_value = np.sum(prob_at * encoder_h, axis = 1)\n","    # v_t = np.concatenate([decoder_h, attention_value], axis = 1)\n","    # s_t = np.tanh(v_t @ Wc.T + bc)\n","    # score = softmax(s_t @ Wy.T + by, axis = 1) \n","    dscore = score * (- np.sum(dloss * score, axis = 1).reshape(N, -1) + dloss)\n","    \n","    dby = dscore.sum(axis = 0)\n","    dWy = dscore.T @ s_t\n","    ds_t = dscore @ Wy\n","    dss_t = (1 - s_t**2) * ds_t\n","    \n","    dbc = dss_t.sum(axis =  0)\n","    dWc = dss_t.T @ v_t\n","\n","\n","\n","    ###it is very difficult\n","    dv_t = dss_t @ Wc\n","    ddh1 = dv_t[:,:D].copy()\n","    dav = dv_t[:,D:]\n","\n","    # deh1 = prob_at * dav\n","    deh1 = np.repeat(np.expand_dims(dav, axis=1), T, axis=1) * prob_at\n","    dprob_at = (np.expand_dims(dav, axis=1) * encoder_h).sum(axis = 2)\n","    prob_at = prob_at.reshape(N,-1)\n","    dscore_at = prob_at* (- np.sum(dprob_at * prob_at, axis = 1).reshape(N, -1) + dprob_at)\n","\n","\n","\n","\n","\n","    deh2 = dscore_at.reshape(N,T,1) * decoder_h.reshape(N,1,D)\n","    ddh2 = (dscore_at.reshape(N,T,1) * encoder_h).sum(axis = 1)\n","\n","\n","    ddh = ddh1 + ddh2\n","    deh = deh1 + deh2\n","    \n","\n","    \n","    \n","\n","    return ddh, deh, dWc, dbc, dWy, dby"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"kBCWwfGFZ9dv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609730763357,"user_tz":-540,"elapsed":2154,"user":{"displayName":"송정윤","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRJcNDmEMhmBD5vygrE-8z7tCUbFYg8knVp6UC=s64","userId":"00034480537731584131"}},"outputId":"1b09ee69-f504-44ab-b4c5-00293dd489ff"},"source":["from gradient_check import *\n","import numpy as np\n","import tensorflow as tf\n","# from attention_layers import attention_step_forward, attention_step_backward\n","N, T, D, word_dim = 3, 5, 6, 10\n","\n","decoder_h = np.linspace(-0.5, 0.5, num=N*D).reshape(N, D)\n","encoder_h = np.linspace(-0.5, 0.5, num=N*T*D).reshape(N, T, D)\n","Wc = np.linspace(-0.1, 0.9, num=2*D**2).reshape(D, 2*D)\n","bc = np.linspace(-0.3, 0.6, num=D).reshape(D, )\n","Wy = np.linspace(-0.3, 0.7, num=word_dim * D).reshape(word_dim,  D)\n","by = np.linspace(-0.2, 0.4, num=word_dim).reshape(word_dim, )\n","\n","next_h, _ = attention_step_forward(decoder_h, encoder_h, Wc, bc, Wy, by)\n","\n","expected_next_h = np.array([[0.33540088, 0.22498186, 0.15091444, 0.10123113, 0.06790432,\n","        0.0455492 , 0.03055371, 0.02049497, 0.01374772, 0.00922176],\n","       [0.04405104, 0.05166217, 0.06058836, 0.0710568 , 0.08333399,\n","        0.09773243, 0.11461862, 0.13442241, 0.1576479 , 0.18488628],\n","       [0.00543037, 0.00870501, 0.01395434, 0.02236912, 0.03585822,\n","        0.05748156, 0.09214426, 0.14770937, 0.23678152, 0.37956623]])\n","\n","print('next_h error: ', rel_error(expected_next_h, next_h))"],"execution_count":9,"outputs":[{"output_type":"stream","text":["next_h error:  3.0950742416212104e-07\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cJwKnCZGpxQn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609730763357,"user_tz":-540,"elapsed":451,"user":{"displayName":"송정윤","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRJcNDmEMhmBD5vygrE-8z7tCUbFYg8knVp6UC=s64","userId":"00034480537731584131"}},"outputId":"0a8a8310-c11a-4250-bf46-dc05406f30b9"},"source":["next_h"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.33540088, 0.22498186, 0.15091444, 0.10123113, 0.06790432,\n","        0.0455492 , 0.03055371, 0.02049497, 0.01374772, 0.00922176],\n","       [0.04405104, 0.05166217, 0.06058836, 0.0710568 , 0.08333399,\n","        0.09773243, 0.11461862, 0.13442241, 0.1576479 , 0.18488628],\n","       [0.00543037, 0.00870501, 0.01395434, 0.02236912, 0.03585822,\n","        0.05748156, 0.09214426, 0.14770937, 0.23678152, 0.37956623]])"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"aFOBQHPMgWCm"},"source":["# Attention : back forward\n","Open the file `attention_layers.py`. This file implements the forward and backward passes for different types of layers that are commonly used in attention."]},{"cell_type":"code","metadata":{"id":"u--i1FCuvLut","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609730764777,"user_tz":-540,"elapsed":910,"user":{"displayName":"송정윤","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRJcNDmEMhmBD5vygrE-8z7tCUbFYg8knVp6UC=s64","userId":"00034480537731584131"}},"outputId":"72c8c304-ae33-4642-e90f-d0b3aeebaa03"},"source":["decoder_h = np.random.randn(N, D)\n","encoder_h = np.random.randn(N, T, D)\n","Wc = np.random.randn(D, 2*D)\n","bc = np.random.randn(D, )\n","Wy = np.random.randn(word_dim, D)\n","by = np.random.randn(word_dim, )\n","sa = np.random.randn(N, T)\n","\n","\n","\n","fdh = lambda decoder_h: attention_step_forward(decoder_h, encoder_h, Wc, bc, Wy, by)[0]\n","feh = lambda encoder_h: attention_step_forward(decoder_h, encoder_h, Wc, bc, Wy, by)[0]\n","fWc = lambda Wc: attention_step_forward(decoder_h, encoder_h, Wc, bc, Wy, by)[0]\n","fbc = lambda bc: attention_step_forward(decoder_h, encoder_h, Wc, bc, Wy, by)[0]\n","fWy = lambda Wy: attention_step_forward(decoder_h, encoder_h, Wc, bc, Wy, by)[0]\n","fby = lambda by: attention_step_forward(decoder_h, encoder_h, Wc, bc, Wy, by)[0]\n","\n","# fsa = lambda vt: attention_step_forward(decoder_h, encoder_h, Wc, bc, Wy, by, sa)[0]\n","\n","score, cache = attention_step_forward(decoder_h, encoder_h, Wc, bc, Wy, by)\n","dnext_h = np.random.randn(*score.shape)\n","\n","ddh_num = eval_numerical_gradient_array(fdh, decoder_h, dnext_h)\n","deh_num = eval_numerical_gradient_array(feh, encoder_h, dnext_h)\n","dWc_num = eval_numerical_gradient_array(fWc, Wc, dnext_h)\n","dbc_num = eval_numerical_gradient_array(fbc, bc, dnext_h)\n","dWy_num = eval_numerical_gradient_array(fWy, Wy, dnext_h)\n","dby_num = eval_numerical_gradient_array(fby, by, dnext_h)\n","# dsa_num = eval_numerical_gradient_array(fsa, sa, dnext_h)\n","\n","\n","ddh, deh, dWc, dbc, dWy, dby = attention_step_backward(dnext_h, cache)\n","\n","print('dby error: ', rel_error(dby_num, dby))\n","print('dWy error: ', rel_error(dWy_num, dWy))\n","print('dbc error: ', rel_error(dbc_num, dbc))\n","print('dWc error: ', rel_error(dWc_num, dWc))\n","print('deh error: ', rel_error(deh_num, deh))\n","print('ddh error: ', rel_error(ddh_num, ddh))"],"execution_count":11,"outputs":[{"output_type":"stream","text":["dby error:  2.0356755939708203e-09\n","dWy error:  1.9415791826442853e-08\n","dbc error:  9.771022052567119e-11\n","dWc error:  5.715093390032916e-09\n","deh error:  5.044453092110602e-08\n","ddh error:  2.1306614439672336e-08\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0fIdC8Xa7apm"},"source":["#Bahdanau Attention"]},{"cell_type":"code","metadata":{"id":"Hfnae-_B7dsg","executionInfo":{"status":"ok","timestamp":1609730765578,"user_tz":-540,"elapsed":827,"user":{"displayName":"송정윤","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRJcNDmEMhmBD5vygrE-8z7tCUbFYg8knVp6UC=s64","userId":"00034480537731584131"}}},"source":["def battention_step_forward(decoder_h, encoder_h, Wa, Wb, Wc, ba, bb, bc):\n","    \n","    \"\"\"\n","    batchsize N\n","\n","    Inputs:\n","    - encoder_h : (N, T, D)\n","    - decoder_h : (N, D) at time t\n","    - Wa : (1, D')\n","    - Wb : (D, D')\n","    - Wc : (D, D')\n","    Returns a tuple of:\n","    - next_h: Next hidden state, of shape (N, H)\n","    - cache: Tuple of values needed for the backward pass.\n","    \"\"\"\n","    N,T,D = encoder_h.shape\n","    _, units = Wb.shape\n","    scores, cache = None, None\n","    score_tanh = np.tanh((decoder_h @ Wb + bb).reshape(N,1,units) + encoder_h @ Wc + bc.reshape(1,1,units))\n","    score_at = score_tanh @ Wa + ba\n","    attention_value = softmax(score_at, axis = 1)\n","    context_vector = np.sum(encoder_h * attention_value, axis = 1)\n","    \n","\n","    cache = score_tanh, score_at, attention_value, decoder_h, encoder_h, Wa, Wb, Wc\n","\n","    return context_vector, cache\n","\n","def battention_step_backward(dloss, cache):\n","    \n","    \"\"\"\n","    batchsize N\n","\n","    Inputs:\n","    - encoder_h : (N, T, D)\n","    - decoder_h : (N, D) at time t\n","    - Wa : (1, D')\n","    - Wb : (D, D')\n","    - Wc : (D, D')\n","    Returns a tuple of:\n","    - next_h: Next hidden state, of shape (N, H)\n","    - cache: Tuple of values needed for the backward pass.\n","    \"\"\"\n","    score_tanh, score_at, attention_value, decoder_h, encoder_h, Wa, Wb, Wc = cache\n","    dWa, dWb, dWc, deh, ddh, dba, dbb, dbc = np.float32(0),np.float32(0),np.float32(0),np.float32(0),np.float32(0),np.float32(0),np.float32(0),np.float32(0)\n","    \n","\n","    N,T,D = encoder_h.shape\n","    D,K = Wb.shape\n","    deh1 = dloss.reshape(N, 1, D) * attention_value.reshape(N, T, 1)\n","    print(deh1.dtype)\n","    dav = np.sum(encoder_h * dloss.reshape(N,1,D), axis = 2)\n","    attention_value = attention_value.reshape(N,T)\n","    dat = attention_value * (dav - np.sum(dav * attention_value, axis = 1).reshape(N, -1))\n","    dWa = np.einsum('NTK, NT-> K', score_tanh, dat)\n","    # dWa = np.sum(dat.reshape(N,T,1) * score_tanh, axis = (0,1))\n","    dba = np.sum(dat)\n","    dscore_tanh = dat.reshape(N,T,1) @ Wa.T\n","    dscore_tanh_in = (1 - score_tanh**2) * dscore_tanh\n","    ddh = np.sum(dscore_tanh_in @ Wb.T, axis = 1)\n","\n","    ddh = np.einsum('NTK, DK -> ND',dscore_tanh_in, Wb)\n","    # ddh = np.sum(dscore_tanh_in @ Wb.T, axis = 1)\n","    deh2 = np.einsum('NTK, DK -> NTD', dscore_tanh_in, Wc)\n","    # deh2 = np.sum(dscore_tanh_in.reshape(N,T,1,K) * Wc.reshape(1,1,D,K), axis = 3)\n","    deh = deh1 + deh2\n","\n","    dWb = np.einsum('ND, NTK -> DK', decoder_h, dscore_tanh_in)\n","    # dWb = np.sum(decoder_h.reshape(N,D,1,1) * dscore_tanh_in.reshape(N,1,T,K), axis = (0,2))\n","    dWc = np.einsum('NTD, NTK -> DK', encoder_h, dscore_tanh_in)\n","    # dWc = np.sum(encoder_h.reshape(N,T,D,1) * dscore_tanh_in.reshape(N,T,1,K), axis = (0,1))\n","    dbb = np.sum(dscore_tanh_in, axis = (0, 1))\n","    dbc = np.sum(dscore_tanh_in, axis = (0, 1))\n","      \n","\n","\n","\n","    return dWa, dWb, dWc, deh, ddh, dba, dbb, dbc"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"O2UKRTAyH1XN","executionInfo":{"status":"ok","timestamp":1609730872362,"user_tz":-540,"elapsed":705,"user":{"displayName":"송정윤","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRJcNDmEMhmBD5vygrE-8z7tCUbFYg8knVp6UC=s64","userId":"00034480537731584131"}}},"source":["\n","# import \n","class BahdanauAttention(tf.keras.Model):\n","  def __init__(self, units):\n","    super(BahdanauAttention, self).__init__()\n","    self.W1 = tf.keras.layers.Dense(units)\n","    self.W2 = tf.keras.layers.Dense(units)\n","    self.V = tf.keras.layers.Dense(1)\n","\n","  def call(self, values, query): # 단, key와 value는 같음\n","    # query shape == (batch_size, hidden size)\n","    # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n","    # score 계산을 위해 뒤에서 할 덧셈을 위해서 차원을 변경해줍니다.\n","    hidden_with_time_axis = tf.expand_dims(query, 1)\n","\n","    # score shape == (batch_size, max_length, 1)\n","    # we get 1 at the last axis because we are applying score to self.V\n","    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n","    score = self.V(tf.nn.tanh(\n","        self.W1(values) + self.W2(hidden_with_time_axis)))\n","    # print(self.W1(values), self.W2(hidden_with_time_axis))\n","    # attention_weights shape == (batch_size, max_length, 1)\n","    attention_weights = tf.nn.softmax(score, axis=1)\n","\n","    # context_vector shape after sum == (batch_size, hidden_size)\n","    context_vector = attention_weights * values\n","    context_vector = tf.reduce_sum(context_vector, axis=1)\n","\n","    return context_vector, attention_weights"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"2TCv6sG7IJic","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609730875013,"user_tz":-540,"elapsed":2280,"user":{"displayName":"송정윤","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRJcNDmEMhmBD5vygrE-8z7tCUbFYg8knVp6UC=s64","userId":"00034480537731584131"}},"outputId":"9f99accf-6e04-40a1-9e8e-dfbbdd48f98d"},"source":["units = 5\n","BahdanauAttention1 = BahdanauAttention(units)\n","tf.random.set_seed(1)\n","BahdanauAttention1(encoder_h, decoder_h)"],"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(<tf.Tensor: shape=(3, 6), dtype=float32, numpy=\n"," array([[ 0.09332979,  0.07215458,  0.09642259, -0.16156846,  0.7399138 ,\n","          0.01438008],\n","        [ 0.23674926,  0.36735293, -0.65598965,  0.46823952,  0.2048228 ,\n","         -0.05906077],\n","        [-0.54972124,  0.2614893 , -0.132893  , -0.5494666 ,  0.2640586 ,\n","          0.42872825]], dtype=float32)>,\n"," <tf.Tensor: shape=(3, 5, 1), dtype=float32, numpy=\n"," array([[[0.14924589],\n","         [0.18572585],\n","         [0.2236181 ],\n","         [0.24923243],\n","         [0.19217783]],\n"," \n","        [[0.16759665],\n","         [0.19448583],\n","         [0.24640141],\n","         [0.19926451],\n","         [0.19225161]],\n"," \n","        [[0.24428836],\n","         [0.21059395],\n","         [0.07079621],\n","         [0.25099948],\n","         [0.223322  ]]], dtype=float32)>)"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"bNTMLVO8MYHx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609730877462,"user_tz":-540,"elapsed":717,"user":{"displayName":"송정윤","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRJcNDmEMhmBD5vygrE-8z7tCUbFYg8knVp6UC=s64","userId":"00034480537731584131"}},"outputId":"27b17341-2e91-4723-a1f3-a969e9c8d2a4"},"source":["    # - encoder_h : (N, T, D)\n","    # - decoder_h : (N, D) at time t\n","    # - Wa : (1, D)\n","    # - Wb : (D, D)\n","    # - Wc : (D, D)\n","Wc, bc, Wb, bb, Wa, ba = BahdanauAttention1.weights[0].numpy(), BahdanauAttention1.weights[1].numpy(), BahdanauAttention1.weights[2].numpy(), BahdanauAttention1.weights[3].numpy(), BahdanauAttention1.weights[4].numpy(), BahdanauAttention1.weights[5].numpy()\n","Wb, bb, Wc, bc, Wa, ba = np.array(Wb, dtype=np.float64), np.array(bb, dtype=np.float64), np.array(Wc, dtype=np.float64), np.array(bc, dtype=np.float64), np.array(Wa, dtype=np.float64), np.array(ba, dtype=np.float64)\n","\n","\n","ba, bb, bc = ba.reshape(1,-1), bb.reshape(1,-1), bc.reshape(1,-1)\n","\n","\n","decoder_h = np.random.randn(N, D)\n","# decoder_h = decoder_h.astype('float32')\n","encoder_h = np.random.randn(N, T, D)\n","# encoder_h = encoder_h.astype('float32')\n","# Wc = np.random.randn(D, units)\n","# Wb = np.random.randn(D, units)\n","# Wa = np.random.randn(units, 1)\n","\n","# ba = np.random.randn(1, 1)\n","# bb = np.random.randn(1, units)\n","# bc = np.random.randn(1, units)\n","fdh = lambda decoder_h: battention_step_forward(decoder_h, encoder_h, Wa, Wb, Wc, ba, bb, bc)[0]\n","feh = lambda encoder_h: battention_step_forward(decoder_h, encoder_h, Wa, Wb, Wc, ba, bb, bc)[0]\n","fWc = lambda Wc: battention_step_forward(decoder_h, encoder_h, Wa, Wb, Wc, ba, bb, bc)[0]\n","fWb = lambda Wb: battention_step_forward(decoder_h, encoder_h, Wa, Wb, Wc, ba, bb, bc)[0]\n","fWa = lambda Wa: battention_step_forward(decoder_h, encoder_h, Wa, Wb, Wc, ba, bb, bc)[0]\n","fba = lambda ba: battention_step_forward(decoder_h, encoder_h, Wa, Wb, Wc, ba, bb, bc)[0]\n","fbb = lambda bb: battention_step_forward(decoder_h, encoder_h, Wa, Wb, Wc, ba, bb, bc)[0]\n","fbc = lambda bc: battention_step_forward(decoder_h, encoder_h, Wa, Wb, Wc, ba, bb, bc)[0]\n","\n","score, cache = battention_step_forward(decoder_h, encoder_h, Wa, Wb, Wc, ba, bb, bc)\n","\n","\n","dnext_h = np.random.randn(*score.shape)\n","ddh_num = eval_numerical_gradient_array(fdh, decoder_h, dnext_h)\n","deh_num = eval_numerical_gradient_array(feh, encoder_h, dnext_h)\n","dWc_num = eval_numerical_gradient_array(fWc, Wc, dnext_h)\n","dWb_num = eval_numerical_gradient_array(fWb, Wb, dnext_h)\n","dWa_num = eval_numerical_gradient_array(fWa, Wa, dnext_h)\n","dba_num = eval_numerical_gradient_array(fba, ba, dnext_h)\n","dbb_num = eval_numerical_gradient_array(fbb, bb, dnext_h)\n","dbc_num = eval_numerical_gradient_array(fbc, bc, dnext_h)\n","\n","\n","dWa, dWb, dWc, deh, ddh, dba, dbb, dbc = battention_step_backward(dnext_h, cache)\n","\n","print('ddh error: ', rel_error(ddh_num, ddh))\n","print('deh error: ', rel_error(deh_num, deh))\n","print('dWc error: ', rel_error(dWc_num, dWc))\n","print('dWb error: ', rel_error(dWb_num, dWb))\n","print('dWa error: ', rel_error(dWa_num.T, dWa))\n","print('dba error: ', rel_error(dba_num.T, dba))\n","print('dbb error: ', rel_error(dbb_num, dbb))\n","print('dbc error: ', rel_error(dbc_num, dbc))"],"execution_count":21,"outputs":[{"output_type":"stream","text":["float64\n","ddh error:  6.170281951912502e-10\n","deh error:  1.517665979660445e-09\n","dWc error:  6.699581029198585e-09\n","dWb error:  2.5840297177755672e-09\n","dWa error:  5.97365757206702e-11\n","dba error:  0.0016299368384956462\n","dbb error:  2.5028512899650587e-09\n","dbc error:  2.5028512899650587e-09\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9hJF17tOu8zi","executionInfo":{"status":"ok","timestamp":1609730882326,"user_tz":-540,"elapsed":718,"user":{"displayName":"송정윤","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRJcNDmEMhmBD5vygrE-8z7tCUbFYg8knVp6UC=s64","userId":"00034480537731584131"}}},"source":["BahdanauAttention1(encoder_h, decoder_h)\n","loss_function = tf.keras.losses.MeanSquaredError()\n","\n","with tf.GradientTape() as tape:\n","  predictions, _ = BahdanauAttention1(tf.convert_to_tensor(encoder_h), tf.convert_to_tensor(decoder_h))\n","  loss = loss_function(np.zeros((N,D)),predictions)\n","\n","gradients = tape.gradient(loss, BahdanauAttention1.weights)"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"id":"xOVa0vg5NJNV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609730882887,"user_tz":-540,"elapsed":509,"user":{"displayName":"송정윤","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRJcNDmEMhmBD5vygrE-8z7tCUbFYg8knVp6UC=s64","userId":"00034480537731584131"}},"outputId":"65661759-2672-4996-f134-4e811cea93aa"},"source":["dnext_h = (2* (predictions)).numpy()/ 18\n","ddh_num = eval_numerical_gradient_array(fdh, decoder_h, dnext_h)\n","deh_num = eval_numerical_gradient_array(feh, encoder_h, dnext_h)\n","dWc_num = eval_numerical_gradient_array(fWc, Wc, dnext_h)\n","dWb_num = eval_numerical_gradient_array(fWb, Wb, dnext_h)\n","dWa_num = eval_numerical_gradient_array(fWa, Wa, dnext_h)\n","dba_num = eval_numerical_gradient_array(fba, ba, dnext_h)\n","dbb_num = eval_numerical_gradient_array(fbb, bb, dnext_h)\n","dbc_num = eval_numerical_gradient_array(fbc, bc, dnext_h)\n","\n","\n","dWa, dWb, dWc, deh, ddh, dba, dbb, dbc = battention_step_backward(dnext_h, cache)\n","\n","print('ddh error: ', rel_error(ddh_num, ddh))\n","print('deh error: ', rel_error(deh_num, deh))\n","print('dWc error: ', rel_error(dWc_num, dWc))\n","print('dWb error: ', rel_error(dWb_num, dWb))\n","print('dWa error: ', rel_error(dWa_num.T, dWa))\n","print('dba error: ', rel_error(dba_num.T, dba))\n","print('dbb error: ', rel_error(dbb_num, dbb))\n","print('dbc error: ', rel_error(dbc_num, dbc))"],"execution_count":23,"outputs":[{"output_type":"stream","text":["float64\n","ddh error:  5.037884062004215e-09\n","deh error:  3.0489782942665384e-09\n","dWc error:  4.227825632401229e-08\n","dWb error:  1.5412739186585457e-08\n","dWa error:  2.0453212351221984e-10\n","dba error:  0.0005603483531667171\n","dbb error:  1.4289432147256335e-09\n","dbc error:  1.4289432147256335e-09\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"oWfejKWaVtlV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609730885395,"user_tz":-540,"elapsed":666,"user":{"displayName":"송정윤","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRJcNDmEMhmBD5vygrE-8z7tCUbFYg8knVp6UC=s64","userId":"00034480537731584131"}},"outputId":"d074164a-b099-43af-f662-9309a1cca1fa"},"source":["print(gradients)\n","\n","print(dWc)\n","print(dbc)\n","print(dWb)\n","print(dbb)\n","print(dWa)\n","print(dba)"],"execution_count":24,"outputs":[{"output_type":"stream","text":["[<tf.Tensor: shape=(6, 5), dtype=float32, numpy=\n","array([[-6.04876913e-02,  1.64003279e-02, -5.66726725e-04,\n","         5.71519835e-03, -1.12971077e-02],\n","       [-8.80641341e-02,  6.04876038e-03,  1.56979228e-03,\n","        -2.63011153e-03, -4.86020072e-05],\n","       [-2.70124935e-02,  1.00945309e-02, -2.99077859e-04,\n","         3.80559242e-03, -3.15385964e-03],\n","       [-5.29027060e-02, -2.64720041e-02, -4.42029850e-05,\n","        -3.15383309e-03,  1.98871060e-03],\n","       [ 5.63869886e-02,  2.80794278e-02,  4.37938422e-03,\n","        -7.49630621e-03,  1.28993765e-02],\n","       [ 3.55078913e-02,  1.82834901e-02,  4.76088887e-03,\n","        -1.06200045e-02,  1.54926116e-02]], dtype=float32)>, <tf.Tensor: shape=(5,), dtype=float32, numpy=\n","array([-0.04020319,  0.01320723,  0.00204353, -0.00243291,  0.00359423],\n","      dtype=float32)>, <tf.Tensor: shape=(6, 5), dtype=float32, numpy=\n","array([[ 5.7762682e-02, -1.1678785e-02, -2.8254007e-04, -6.3083065e-03,\n","         9.9645685e-03],\n","       [-2.4183257e-02, -4.0865964e-03,  5.2997260e-04,  1.1197475e-04,\n","        -1.5425500e-03],\n","       [-8.8289194e-02,  4.0384000e-03,  2.7425434e-03, -9.0617104e-04,\n","        -1.4938561e-03],\n","       [ 4.7512017e-02, -9.2353495e-03,  1.8067949e-04, -6.7912885e-03,\n","         1.0572679e-02],\n","       [ 8.3058998e-02,  9.7473478e-03, -3.5035412e-03,  5.8685839e-03,\n","        -4.3085869e-03],\n","       [ 1.5871270e-01, -1.5226243e-02, -5.8150250e-03,  4.3465230e-03,\n","        -2.2052189e-03]], dtype=float32)>, <tf.Tensor: shape=(5,), dtype=float32, numpy=\n","array([-0.04020319,  0.01320723,  0.00204353, -0.00243291,  0.00359423],\n","      dtype=float32)>, <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n","array([[ 0.21352936],\n","       [-0.09612741],\n","       [-0.07188936],\n","       [ 0.00454238],\n","       [ 0.00388339]], dtype=float32)>, <tf.Tensor: shape=(1,), dtype=float32, numpy=array([5.3551048e-08], dtype=float32)>]\n","[[-6.04877127e-02  1.64003088e-02 -5.66727405e-04  5.71520180e-03\n","  -1.12971057e-02]\n"," [-8.80641264e-02  6.04875606e-03  1.56979132e-03 -2.63011227e-03\n","  -4.86024558e-05]\n"," [-2.70125121e-02  1.00945215e-02 -2.99078432e-04  3.80559139e-03\n","  -3.15385892e-03]\n"," [-5.29026905e-02 -2.64719898e-02 -4.42028504e-05 -3.15383632e-03\n","   1.98871019e-03]\n"," [ 5.63869925e-02  2.80794267e-02  4.37938331e-03 -7.49630459e-03\n","   1.28993797e-02]\n"," [ 3.55079017e-02  1.82834875e-02  4.76088790e-03 -1.06200044e-02\n","   1.54926122e-02]]\n","[-0.0402032   0.01320723  0.00204353 -0.00243291  0.00359423]\n","[[ 5.77627058e-02 -1.16787644e-02 -2.82539702e-04 -6.30830099e-03\n","   9.96456722e-03]\n"," [-2.41832541e-02 -4.08659687e-03  5.29972653e-04  1.11972917e-04\n","  -1.54255016e-03]\n"," [-8.82892070e-02  4.03839108e-03  2.74254232e-03 -9.06174162e-04\n","  -1.49385448e-03]\n"," [ 4.75120378e-02 -9.23532995e-03  1.80679544e-04 -6.79128330e-03\n","   1.05726787e-02]\n"," [ 8.30589900e-02  9.74734193e-03 -3.50354062e-03  5.86858614e-03\n","  -4.30858787e-03]\n"," [ 1.58712720e-01 -1.52262269e-02 -5.81502217e-03  4.34652553e-03\n","  -2.20522292e-03]]\n","[-0.0402032   0.01320723  0.00204353 -0.00243291  0.00359423]\n","[ 0.21352933 -0.09612738 -0.07188938  0.00454244  0.00388345]\n","-3.469446951953614e-17\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5W8LMjoWbUX4"},"source":[""],"execution_count":null,"outputs":[]}]}